{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Data mining \n",
    "## Text mining et NLP\n",
    "## Université Paris Saclay\n",
    "## Master Innovation, Marchés et Science des Données\n",
    "### Promotion 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\57621\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report,confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import pipeline\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiser des stopwords\n",
    "np.random.seed(500)\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopwords_forCloud = set(STOPWORDS)\n",
    "stopwords_forCloud.update(['flight', 'flights', 'Flightled', 'AmericanAir', 'VirginAmerica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation du datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_doc2vec</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>what say</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>plus youve added commercial to the experience ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i didnt today must mean i need to take another...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>it really aggressive to blast obnoxious entert...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>and it a really big bad thing about it</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 airline_sentiment         airline negativereason  \\\n",
       "0           0           neutral  Virgin America            NaN   \n",
       "1           1          positive  Virgin America            NaN   \n",
       "2           2           neutral  Virgin America            NaN   \n",
       "3           3          negative  Virgin America     Bad Flight   \n",
       "4           4          negative  Virgin America     Can't Tell   \n",
       "\n",
       "                                                text  label  label_doc2vec  \\\n",
       "0                @VirginAmerica What @dhepburn said.      0              1   \n",
       "1  @VirginAmerica plus you've added commercials t...      1              2   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...      0              1   \n",
       "3  @VirginAmerica it's really aggressive to blast...     -1              0   \n",
       "4  @VirginAmerica and it's a really big bad thing...     -1              0   \n",
       "\n",
       "                                      processed_text  processed_text_length  \n",
       "0                                           what say                      2  \n",
       "1  plus youve added commercial to the experience ...                      8  \n",
       "2  i didnt today must mean i need to take another...                     11  \n",
       "3  it really aggressive to blast obnoxious entert...                     16  \n",
       "4             and it a really big bad thing about it                      9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "airline_kaggle = pd.read_csv('.\\\\Tweets_kaggle_processed.csv')\n",
    "airline_kaggle.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We hear you and understand the concerns you ha...</td>\n",
       "      <td>we hear you and understand the concern you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you for helping @united!</td>\n",
       "      <td>thank you for help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I filed a claim with United. As I out in my mi...</td>\n",
       "      <td>i file a claim with united a i out in my milea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@msdunn_says</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delta is waiving change fee on tickets booked ...</td>\n",
       "      <td>delta be waive change fee on ticket book befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  We hear you and understand the concerns you ha...   \n",
       "1                     Thank you for helping @united!   \n",
       "2  I filed a claim with United. As I out in my mi...   \n",
       "3                                       @msdunn_says   \n",
       "4  Delta is waiving change fee on tickets booked ...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  we hear you and understand the concern you hav...  \n",
       "1                                 thank you for help  \n",
       "2  i file a claim with united a i out in my milea...  \n",
       "3                                                say  \n",
       "4  delta be waive change fee on ticket book befor...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_scrappe = pd.read_csv(\".\\\\Tweet_scrape_processed.csv\")\n",
    "airline_scrappe = airline_scrappe.loc[:,[\"text\",\"processed_text\"]]\n",
    "airline_scrappe = airline_scrappe.dropna()\n",
    "airline_scrappe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection des colonnes qui seront utilisées pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>plus youve added commercial to the experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i didnt today must mean i need to take another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>it really aggressive to blast obnoxious entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>and it a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>seriously would pay a flight for seat that did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes nearly every time i fly vx this ear worm w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>really miss a prime opportunity for men withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well i didnt but now i do d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it be amaze and arrive an hour early youre too...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline negativereason  \\\n",
       "0           neutral  Virgin America            NaN   \n",
       "1          positive  Virgin America            NaN   \n",
       "2           neutral  Virgin America            NaN   \n",
       "3          negative  Virgin America     Bad Flight   \n",
       "4          negative  Virgin America     Can't Tell   \n",
       "5          negative  Virgin America     Can't Tell   \n",
       "6          positive  Virgin America            NaN   \n",
       "7           neutral  Virgin America            NaN   \n",
       "8          positive  Virgin America            NaN   \n",
       "9          positive  Virgin America            NaN   \n",
       "\n",
       "                                      processed_text  \n",
       "0                                           what say  \n",
       "1  plus youve added commercial to the experience ...  \n",
       "2  i didnt today must mean i need to take another...  \n",
       "3  it really aggressive to blast obnoxious entert...  \n",
       "4             and it a really big bad thing about it  \n",
       "5  seriously would pay a flight for seat that did...  \n",
       "6  yes nearly every time i fly vx this ear worm w...  \n",
       "7  really miss a prime opportunity for men withou...  \n",
       "8                        well i didnt but now i do d  \n",
       "9  it be amaze and arrive an hour early youre too...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "airline_sub = airline_kaggle.loc[:, ['airline_sentiment', 'airline', 'negativereason', 'processed_text']]\n",
    "airline_sub.head(10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour la création des graphiques de nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        #contour_width = 3, \n",
    "        #contour_color = 'steelblue',\n",
    "        stopwords = stopwords_forCloud,\n",
    "        max_words = 80,\n",
    "        max_font_size = 50, \n",
    "        scale = 3,\n",
    "        random_state = 1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    plt.figure(1, figsize = (10, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour lemmatiser les texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def lemmatize_text(text):    \n",
    "    lmtzr = WordNetLemmatizer().lemmatize\n",
    "    text = word_tokenize(str(text))   # Init the Wordnet Lemmatizer    \n",
    "    word_pos = pos_tag(text)    \n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "    return (' '.join(lemm_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de nettoyage de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why be the site down when will it be back up'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def pre_process(text):       \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)    \n",
    "\n",
    "    text = emoji_pattern.sub(r'', text)                       # remove emojis       \n",
    "    text = text.lower()                                       # lower all letters   \n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)                 # remove user mentions such as @VirginAmerica    \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)       # remove URL\n",
    "\n",
    "    text = ''.join([t for t in text if t not in string.punctuation])   # remove punctuations       \n",
    "    text = ''.join([t for t in text if not t.isdigit()])   # remove numeric digits     \n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)   # only leave letters\n",
    "    text = lemmatize_text(text)   # use Wordnet(lexical database) to lemmatize text     \n",
    "    return text\n",
    "\n",
    "##Test one of the messages\n",
    "pre_process(airline_sub[\"processed_text\"][89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créaction de la colonne label qui est la colonne cible en le valorisant -1,0,1 qui représentent Negative: -1, neutral: 0, positive 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline_sub sentiment_count:  negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>plus youve added commercial to the experience ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i didnt today must mean i need to take another...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>it really aggressive to blast obnoxious entert...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>and it a really big bad thing about it</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline negativereason  \\\n",
       "0           neutral  Virgin America            NaN   \n",
       "1          positive  Virgin America            NaN   \n",
       "2           neutral  Virgin America            NaN   \n",
       "3          negative  Virgin America     Bad Flight   \n",
       "4          negative  Virgin America     Can't Tell   \n",
       "\n",
       "                                      processed_text  label  \n",
       "0                                           what say      0  \n",
       "1  plus youve added commercial to the experience ...      1  \n",
       "2  i didnt today must mean i need to take another...      0  \n",
       "3  it really aggressive to blast obnoxious entert...     -1  \n",
       "4             and it a really big bad thing about it     -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#créer la colonne label qui est la colonne cible en le valorisant -1,0,1 qui représentent Negative: -1, neutral: 0, positive 1\n",
    "sentiment_count = airline_sub['airline_sentiment'].value_counts()  # negative 9178, neutral 3099, positive 2363\n",
    "print(\"airline_sub sentiment_count: \", sentiment_count)\n",
    "\n",
    "airline_sub['label'] = airline_sub['airline_sentiment'].map({'negative': -1, 'neutral': 0, 'positive': 1}) \n",
    "\n",
    "#airline_sub['processed_text'] =  airline_sub['text'].apply(pre_process)  \n",
    "#airline_sub['processed_text_length'] = airline_sub['processed_text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "airline_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour la visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fonction de visualiser nos données\n",
    "def Airline_Tweet_Visualization():\n",
    "    \n",
    "    # numbers of neutral, positive and neguative reviews.\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.countplot(x = 'airline_sentiment', data = airline_sub, order = airline_sub['airline_sentiment'].value_counts().index, palette = 'Set1')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # numbers of each airline for the frequency of sentiment\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.countplot(x = 'airline', data = airline_sub, hue = 'airline_sentiment', order = airline_sub['airline'].value_counts().index, palette = 'Set2')\n",
    "    plt.xlabel('Airline')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend().set_title('Sentiment')\n",
    "    plt.show()\n",
    "            \n",
    "    # frequency of the different negative reasons          \n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.countplot(y = 'negativereason', data = airline_sub, order = airline_sub['negativereason'].value_counts().index, palette = 'Set3')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Negative Reason')\n",
    "    plt.show()\n",
    "    \n",
    "    # distribution of negative reasons on each airlines\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x = 'airline', data = airline_sub, hue = 'negativereason', palette = 'Set3', saturation = True)\n",
    "    plt.xlabel('Airline companies')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(bbox_to_anchor = (1.01, 1), loc = 2, borderaxespad = 0.1)\n",
    "    plt.show() \n",
    "    \n",
    "    # review length of distribution over neutral, positive and negative sentiment\n",
    "    sns.boxplot(x = 'airline_sentiment', y = 'processed_text_length', data = airline_sub)    \n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Processed Text Length')\n",
    "    plt.ylim(0, 50)\n",
    "    plt.show()   \n",
    "    \n",
    "    # wordcloud for negative, neutral and positive tweets\n",
    "    airline_sub_neg = airline_sub.loc[airline_sub['airline_sentiment'] == 'negative']   \n",
    "    airline_sub_neu = airline_sub.loc[airline_sub['airline_sentiment'] == 'neutral']    \n",
    "    airline_sub_pos = airline_sub.loc[airline_sub['airline_sentiment'] == 'positive']   \n",
    "\n",
    "    # show world cloud with negative sentiment\n",
    "    print('========================world cloud with negative sentiment==================================')\n",
    "    show_wordcloud(airline_sub_neg['processed_text'])   \n",
    "    # show world cloud with neutral sentiment\n",
    "    print('========================world cloud with neutral sentiment==================================')\n",
    "    show_wordcloud(airline_sub_neu['processed_text'])     \n",
    "    # show world cloud with positive sentiment  \n",
    "    print('========================world cloud with positive sentiment==================================')\n",
    "    show_wordcloud(airline_sub_pos['processed_text'])     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rééchatillonnage des données (sur-échantillonnage et sous-échatillonage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(train_X):\n",
    "    df_major_neg = train_X[train_X['label'] == -1]\n",
    "    df_minor_neu = train_X[train_X['label'] == 0]\n",
    "    df_minor_pos = train_X[train_X['label'] == 1]        \n",
    "    major_count = len(df_major_neg)\n",
    " \n",
    "    # oversample minority class\n",
    "    df_minor_neu_oversampled = resample(df_minor_neu, \n",
    "                                 replace = True,              # sample with replacement\n",
    "                                 n_samples = major_count,     # to match majority class \n",
    "                                 random_state = 1000)    \n",
    "\n",
    "    df_minor_pos_oversampled = resample(df_minor_pos, \n",
    "                                 replace = True,             \n",
    "                                 n_samples = major_count,   \n",
    "                                 random_state = 1000)      \n",
    "         \n",
    "    trainX = pd.concat([df_major_neg, df_minor_neu_oversampled, df_minor_pos_oversampled])   # Combine majority class with oversampled minority class\n",
    "    print(\"Train dataset calss distribution: \\n\", trainX.label.value_counts())\n",
    "    trainX = shuffle(trainX, random_state = 200) \n",
    "    return trainX\n",
    "\n",
    "def undersampling(train_X):\n",
    "    df_major_neg = train_X[train_X['label'] == -1]\n",
    "    df_minor_neu = train_X[train_X['label'] == 0]\n",
    "    df_minor_pos = train_X[train_X['label'] == 1]        \n",
    "    minor_count = len(df_minor_pos)\n",
    " \n",
    "    # undersample minority class\n",
    "    df_major_neg_undersampled = resample(df_major_neg, \n",
    "                                 replace = True,              # sample with replacement\n",
    "                                 n_samples = minor_count,     # to match minority class\n",
    "                                 random_state = 1000)    \n",
    "\n",
    "    df_minor_neu_undersampled = resample(df_minor_neu, \n",
    "                                 replace = True,             \n",
    "                                 n_samples = minor_count,   \n",
    "                                 random_state = 1000)      \n",
    "         \n",
    "    trainX = pd.concat([df_major_neg_undersampled, df_minor_neu_undersampled, df_minor_pos])   # Combine majority class with oversampled minority class\n",
    "    print(\"Train dataset calss distribution: \\n\", trainX.label.value_counts())\n",
    "    trainX = shuffle(trainX, random_state = 200) \n",
    "    return trainX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_NB(df, feature, ngram, sample_method):    \n",
    "    random.seed(999)\n",
    "        \n",
    "    if feature == \"TF\":\n",
    "        vector = CountVectorizer(analyzer = 'word', ngram_range=(1, ngram)) \n",
    "    elif feature == \"TFIDF\":        \n",
    "        vector = TfidfVectorizer(analyzer = 'word', ngram_range=(1, ngram))\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(df, df['label'], test_size = 0.3, random_state = 222)\n",
    "        \n",
    "    if sample_method == \"undersampling\":\n",
    "        train_X = undersampling(train_X)\n",
    "    \n",
    "    elif sample_method == \"oversampling\":    \n",
    "        train_X = oversampling(train_X)   \n",
    "          \n",
    "    pipe = make_pipeline(vector, MultinomialNB(alpha = 1.0, fit_prior = True))\n",
    "    pipe.fit(train_X['processed_text'], train_X['label'])          \n",
    "    \n",
    "    test_y_hat = pipe.predict(test_X['processed_text'])\n",
    "    \n",
    "    df_result = test_X.copy()\n",
    "    df_result['prediction'] = test_y_hat.tolist()  \n",
    "    \n",
    "    df_prob = pd.DataFrame(pipe.predict_proba(test_X['processed_text']), columns = pipe.classes_)\n",
    "    df_prob.index = df_result.index\n",
    "    df_prob.columns = ['probability_negative', 'Probability_neutral', 'probability_positive']\n",
    "\n",
    "    df_final = pd.concat([df_result, df_prob], axis = 1)\n",
    "    \n",
    "    #file_name = 'NB_' + str(ngram) + '_' + sample_method \n",
    "    #df_final.to_csv(file_name + '.csv')       \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"NB classification report -- \", \"feature: %s/\" %feature, \"ngram: %d/\" %ngram, \"sample_method: %s/\" %sample_method)\n",
    "    print(pd.crosstab(test_y.ravel(), test_y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True))      \n",
    "\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(classification_report(test_y, test_y_hat))    \n",
    "    print('Macro F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'macro')))  \n",
    "    print('Weighted F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'weighted')))    \n",
    "    joblib.dump(pipe, \"NB_\"+\"train_model.m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_SVM(df, feature, ngram, sample_method):    \n",
    "    random.seed(888)\n",
    "        \n",
    "    if feature == \"TF\":\n",
    "        vector = CountVectorizer(analyzer = 'word', ngram_range=(1, ngram)) \n",
    "    elif feature == \"TFIDF\":        \n",
    "        vector = TfidfVectorizer(analyzer = 'word', ngram_range=(1, ngram))\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(df, df['label'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "    if sample_method == \"undersampling\":\n",
    "        train_X = undersampling(train_X)\n",
    "    \n",
    "    elif sample_method == \"oversampling\":    \n",
    "        train_X = oversampling(train_X)               \n",
    " \n",
    "    pipe = make_pipeline(vector, svm.SVC(kernel = 'linear', probability = True, random_state = 123))\n",
    "    pipe.fit(train_X['processed_text'], train_X['label'])     \n",
    "    \n",
    "    test_y_hat = pipe.predict(test_X['processed_text'])\n",
    "        \n",
    "    df_result = test_X.copy()\n",
    "    df_result['prediction'] = test_y_hat.tolist()   \n",
    "    \n",
    "    df_prob = pd.DataFrame(pipe.predict_proba(test_X['processed_text']), columns = pipe.classes_)\n",
    "    df_prob.index = df_result.index\n",
    "    df_prob.columns = ['probability_negative', 'Probability_neutral', 'probability_positive']\n",
    "\n",
    "    df_final = pd.concat([df_result, df_prob], axis = 1)\n",
    "    \n",
    "#     file_name = 'SVM_' + str(ngram) + '_' + sample_method \n",
    "#     df_final.to_csv(file_name + '.csv')       \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"SVM classification report -- \", \"feature: %s/\" %feature, \"ngram: %d/\" %ngram, \"sample_method: %s/\" %sample_method)\n",
    "    print(pd.crosstab(test_y.ravel(), test_y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True))  \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(classification_report(test_y, test_y_hat))\n",
    "    \n",
    "    print('Macro F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'macro')))  \n",
    "    print('Weighted F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'weighted')))\n",
    "    joblib.dump(pipe, \"SVM_\"+\"train_model.m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_random_forest(df, feature, ngram, sample_method):    \n",
    "    random.seed(888)\n",
    "        \n",
    "    if feature == \"TF\":\n",
    "        vector = CountVectorizer(analyzer = 'word', ngram_range=(1, ngram)) \n",
    "    elif feature == \"TFIDF\":        \n",
    "        vector = TfidfVectorizer(analyzer = 'word', ngram_range=(1, ngram))\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(df, df['label'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "    if sample_method == \"undersampling\":\n",
    "        train_X = undersampling(train_X)\n",
    "    \n",
    "    elif sample_method == \"oversampling\":    \n",
    "        train_X = oversampling(train_X)               \n",
    " \n",
    "    pipe = make_pipeline(vector, RandomForestClassifier(n_estimators=100, random_state=0))\n",
    "    pipe.fit(train_X['processed_text'], train_X['label'])     \n",
    "    \n",
    "    test_y_hat = pipe.predict(test_X['processed_text'])\n",
    "        \n",
    "    df_result = test_X.copy()\n",
    "    df_result['prediction'] = test_y_hat.tolist()   \n",
    "    \n",
    "    df_prob = pd.DataFrame(pipe.predict_proba(test_X['processed_text']), columns = pipe.classes_)\n",
    "    df_prob.index = df_result.index\n",
    "    df_prob.columns = ['probability_negative', 'Probability_neutral', 'probability_positive']\n",
    "\n",
    "    df_final = pd.concat([df_result, df_prob], axis = 1)\n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"random forest -- \", \"feature: %s/\" %feature, \"ngram: %d/\" %ngram, \"sample_method: %s/\" %sample_method)\n",
    "    print(pd.crosstab(test_y.ravel(), test_y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True))  \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(classification_report(test_y, test_y_hat))\n",
    "    \n",
    "    print('Macro F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'macro')))  \n",
    "\n",
    "    print('Weighted F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'weighted'))) \n",
    "    joblib.dump(pipe, \"RF_\"+\"train_model.m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(df, feature, ngram, sample_method):    \n",
    "    random.seed(888)\n",
    "        \n",
    "    if feature == \"TF\":\n",
    "        vector = CountVectorizer(analyzer = 'word', ngram_range=(1, ngram)) \n",
    "    elif feature == \"TFIDF\":        \n",
    "        vector = TfidfVectorizer(analyzer = 'word', ngram_range=(1, ngram))\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(df, df['label'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "    if sample_method == \"undersampling\":\n",
    "        train_X = undersampling(train_X)\n",
    "    \n",
    "    elif sample_method == \"oversampling\":    \n",
    "        train_X = oversampling(train_X)               \n",
    " \n",
    "    pipe = make_pipeline(vector, MLPClassifier( solver='lbfgs', random_state = 0,hidden_layer_sizes=[100]))\n",
    "    pipe.fit(train_X['processed_text'], train_X['label'])     \n",
    "    \n",
    "    test_y_hat = pipe.predict(test_X['processed_text'])\n",
    "        \n",
    "    df_result = test_X.copy()\n",
    "    df_result['prediction'] = test_y_hat.tolist()   \n",
    "    \n",
    "    df_prob = pd.DataFrame(pipe.predict_proba(test_X['processed_text']), columns = pipe.classes_)\n",
    "    df_prob.index = df_result.index\n",
    "    df_prob.columns = ['probability_negative', 'Probability_neutral', 'probability_positive']\n",
    "\n",
    "    df_final = pd.concat([df_result, df_prob], axis = 1)\n",
    "    \n",
    "     \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"MLP classification report -- \", \"feature: %s/\" %feature, \"ngram: %d/\" %ngram, \"sample_method: %s/\" %sample_method)\n",
    "    print(pd.crosstab(test_y.ravel(), test_y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True))  \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(classification_report(test_y, test_y_hat))\n",
    "    \n",
    "    print('Macro F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'macro')))  \n",
    "    print('Weighted F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'weighted'))) \n",
    "    joblib.dump(pipe, \"mlp_\"+\"train_model.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(df, feature, ngram, sample_method):    \n",
    "    random.seed(888)\n",
    "        \n",
    "    if feature == \"TF\":\n",
    "        vector = CountVectorizer(analyzer = 'word', ngram_range=(1, ngram)) \n",
    "    elif feature == \"TFIDF\":        \n",
    "        vector = TfidfVectorizer(analyzer = 'word', ngram_range=(1, ngram))\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(df, df['label'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "    if sample_method == \"undersampling\":\n",
    "        train_X = undersampling(train_X)\n",
    "    \n",
    "    elif sample_method == \"oversampling\":    \n",
    "        train_X = oversampling(train_X)               \n",
    " \n",
    "    pipe = make_pipeline(vector, MLPClassifier( solver='lbfgs',activation='tanh', random_state = 0,hidden_layer_sizes=[100]))\n",
    "    pipe.fit(train_X['processed_text'], train_X['label'])    \n",
    "    \n",
    "    test_y_hat = pipe.predict(test_X['processed_text'])\n",
    "        \n",
    "    df_result = test_X.copy()\n",
    "    df_result['prediction'] = test_y_hat.tolist()   \n",
    "    \n",
    "    df_prob = pd.DataFrame(pipe.predict_proba(test_X['processed_text']), columns = pipe.classes_)\n",
    "    df_prob.index = df_result.index\n",
    "    df_prob.columns = ['probability_negative', 'Probability_neutral', 'probability_positive']\n",
    "\n",
    "    df_final = pd.concat([df_result, df_prob], axis = 1)\n",
    "    \n",
    "#     file_name = 'SVM_' + str(ngram) + '_' + sample_method \n",
    "#     df_final.to_csv(file_name + '.csv')       \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"MLP classification report -- \", \"feature: %s/\" %feature, \"ngram: %d/\" %ngram, \"sample_method: %s/\" %sample_method)\n",
    "    print(pd.crosstab(test_y.ravel(), test_y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True))  \n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(classification_report(test_y, test_y_hat))\n",
    "    \n",
    "    print('Macro F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'macro')))  \n",
    "    print('Weighted F1 Score: {:.2f}'.format(f1_score(test_y_hat, test_y, average = 'weighted'))) \n",
    "    joblib.dump(pipe, \"mlp_\"+\"train_model.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    \n",
    "    airline_model = airline_kaggle.loc[:, ['processed_text', 'label']]\n",
    "    #airline_model.to_csv('airline_model.csv')  \n",
    "            \n",
    "    # Naive Bayes. Arguments: dataframe, TF/IFIDF, unigran or ngram, data-balancing method   \n",
    "    \n",
    "    for i in range(1,4):\n",
    "        airline_NB(airline_model, \"TFIDF\", i, \"none\")  \n",
    "        airline_NB(airline_model, \"TFIDF\", i, \"oversampling\")  \n",
    "        airline_NB(airline_model, \"TFIDF\", i, \"undersampling\")    \n",
    "    \n",
    "    # SVM. Arguments: dataframe, TF/IFIDF, unigran or ngram, data-balancing method    \n",
    "    for i in range(1,4):\n",
    "        airline_SVM(airline_model, \"TFIDF\", i, \"none\")  \n",
    "        airline_SVM(airline_model, \"TFIDF\", i, \"oversampling\")  \n",
    "        airline_SVM(airline_model, \"TFIDF\", i, \"undersampling\")  \n",
    "        \n",
    "    # RandomForest : Arguments: dataframe, TF/IFIDF, unigran or ngram, data-balancing method\n",
    "    for i in range(1,4):\n",
    "        airline_random_forest(airline_model, \"TFIDF\", i, \"none\")  \n",
    "        airline_random_forest(airline_model, \"TFIDF\", i, \"oversampling\")  \n",
    "        airline_random_forest(airline_model, \"TFIDF\",i, \"undersampling\") \n",
    "    \n",
    "    # MLP : : Arguments: dataframe, TF/IFIDF, unigran or ngram, data-balancing method\n",
    "    for i in range(1,4):\n",
    "        MLP(airline_model, \"TFIDF\", i, \"none\")  \n",
    "        MLP(airline_model, \"TFIDF\", i, \"oversampling\")  \n",
    "        MLP(airline_model, \"TFIDF\",i, \"undersampling\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliquer le modèle obtenu, faire la prédiction sur le dataset de scrappe et stock le résultat en CSV\n",
    "clf = joblib.load(\"train_model.m\")\n",
    "airline_scrappe[\"label\"] = clf.predict(airline_scrappe[\"processed_text\"])\n",
    "airline_scrappe.to_csv(\"airline_scrappe_avec_label.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
